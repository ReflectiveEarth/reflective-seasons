{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ec00d68-7dce-4147-8f99-e5d2faaf1470",
   "metadata": {},
   "source": [
    "# Data Ingest\n",
    "\n",
    "This notebook contains a workflow to:\n",
    "\n",
    "1. Download meteorological reanalysis data from the Copernicus [Climate Data Store](https://cds.climate.copernicus.eu/) and\n",
    "2. Upload the data to a Google Cloud Storage bucket.\n",
    "\n",
    "This data ingest is necessary to support an analysis of Earth's radiation budget under clear sky conditions concurrent with the hottest week of the year, so we will require five available variables:\n",
    "\n",
    "* Top of Atmosphere Incident Solar Radiation (`toa_incident_solar_radiation`)\n",
    "* Top of Atmosphere Net Solar Radiation, Clear Sky (`top_net_solar_radiation_clear_sky`)\n",
    "* Surface Solar Radiation Downward, Clear Sky (`surface_solar_radiation_downward_clear_sky`)\n",
    "* Surface Net Solar Radiation, Clear Sky (`surface_net_solar_radiation_clear_sky`)\n",
    "* 2m Temperature (`2m_temperature`)\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "### Requirements\n",
    "* A Copernicus Climate Data Store account ([Create new account](https://cds.climate.copernicus.eu/user/register))\n",
    "* A Google Cloud project with Cloud Storage enabled ([Create new account](https://cloud.google.com/))\n",
    "* Python packages. See `environments` directory for platform specific environment files.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fd15e4-14c1-4469-9f31-3c37bc399211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import check_environment\n",
    "\n",
    "check_environment(\"ingest\")\n",
    "\n",
    "import contextlib\n",
    "from datetime import timedelta, date\n",
    "from functools import wraps\n",
    "import logging\n",
    "import multiprocessing\n",
    "from os import environ\n",
    "import os\n",
    "from sys import platform\n",
    "import urllib.request\n",
    "\n",
    "import cdsapi\n",
    "import certifi\n",
    "from google.cloud import storage\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "import urllib3\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19311a32-dd80-4402-a5d4-4657a748cdc9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750a44a1-44f2-4712-923a-42f81f0830c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(1990, 1, 1)\n",
    "end_date = date(1990, 1, 5)\n",
    "hourly_data_bucket = \"era5-single-level\"\n",
    "n_jobs = 1  # number of jobs for parallelization; if 1, then serial; if negative, then (n_cpus + 1 + n_jobs) are used\n",
    "\n",
    "# ECMWF C3S CDS Client Configuration\n",
    "try:\n",
    "    if None not in (environ[\"CDSAPI_URL\"], environ[\"CDSAPI_KEY\"]):\n",
    "        pass\n",
    "except KeyError:\n",
    "    # Fallback to .cdsapirc\n",
    "    try:\n",
    "        with open(os.path.join(os.path.expanduser(\"~\"),\".cdsapirc\"), \"r\") as f:\n",
    "            output = f.read()\n",
    "    except IOError:\n",
    "        logging.warning(\"No $CDSAPI_URL, $CDSAPI_KEY, or .cdsapirc found.\")\n",
    "\n",
    "# Certificate management\n",
    "http = urllib3.PoolManager(\n",
    "    cert_reqs='CERT_REQUIRED',\n",
    "    ca_certs=certifi.where()\n",
    ")\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(filename=\"ingest.log\", filemode=\"w\", level=logging.INFO)\n",
    "\n",
    "# Multiprocessing configuration for MacOS\n",
    "if platform == \"darwin\":\n",
    "    multiprocessing.set_start_method(\"fork\", force=True)  # ipython bug workaround https://github.com/ipython/ipython/issues/12396\n",
    "    \n",
    "# Project ID\n",
    "url = \"http://metadata.google.internal/computeMetadata/v1/project/project-id\"\n",
    "req = urllib.request.Request(url)\n",
    "req.add_header(\"Metadata-Flavor\", \"Google\")\n",
    "project_id = urllib.request.urlopen(req).read().decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eaca61-34e5-426e-ac24-63d075bc1608",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c8762a-c93d-478a-bdf1-c0d52c7b28a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry(f):\n",
    "    \"\"\"Retry wrapper function used as a decorator.\"\"\"\n",
    "    @wraps(f)\n",
    "    def wrapped(*args, **kwargs):\n",
    "        while True:\n",
    "            try:\n",
    "                return f(*args, **kwargs)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Patch joblib to report into tqdm progress bar given as argument.\"\"\"\n",
    "\n",
    "    def tqdm_print_progress(self):\n",
    "        if self.n_completed_tasks > tqdm_object.n:\n",
    "            n_completed = self.n_completed_tasks - tqdm_object.n\n",
    "            tqdm_object.update(n=n_completed)\n",
    "\n",
    "    original_print_progress = joblib.parallel.Parallel.print_progress\n",
    "    joblib.parallel.Parallel.print_progress = tqdm_print_progress\n",
    "\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.Parallel.print_progress = original_print_progress\n",
    "        tqdm_object.close()\n",
    "\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    \"\"\"Make a date range object spanning two dates.\n",
    "    \n",
    "    Args:\n",
    "      start_date: date object to start from.\n",
    "      end_date: date object to end at.\n",
    "    \n",
    "    Yields:\n",
    "      date object for iteration.\n",
    "    \"\"\"\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "\n",
    "def get_file_gcs(file_name, bucket_name, project_name=None, local_path=\".\"):\n",
    "    \"\"\"Download a file from Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): file_name to download from gcs.\n",
    "        bucket_name (str): Google Cloud Storage bucket to download from.\n",
    "        project_name (str): Google Cloud project name.\n",
    "        local_path (str): optional local path to download to.\n",
    "\n",
    "    Returns:\n",
    "        Nothing; downloads a file from Google Cloud Storage.\n",
    "    \"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name, user_project=project_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "    blob.download_to_filename(filename=os.path.join(local_path, file_name))\n",
    "\n",
    "\n",
    "@retry\n",
    "def put_file_gcs(file_name, bucket_name, project_name, local_path=\".\"):\n",
    "    \"\"\"Upload a dataset for a single date to Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): file name to upload to Google Cloud Storage.\n",
    "        bucket_name (str): Google Cloud Storage bucket to download from.\n",
    "        project_name (str): Google Cloud project name.\n",
    "        local_path (str): optional local path to download to.\n",
    "\n",
    "    Returns:\n",
    "        Nothing; uploads data to Google Cloud Storage.\n",
    "    \"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name, user_project=project_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "    blob.upload_from_filename(filename=os.path.join(local_path, file_name))\n",
    "    \n",
    "\n",
    "def check_blob_size(file_name, bucket_name, project_name=None, raise_threshold=1e+2):\n",
    "    \"\"\"Verify that a GCS blob is larger than a specified threshold.\n",
    "    \n",
    "    Args:\n",
    "        single_date (date): day to retrieve data for.\n",
    "        bucket_name (str): Google Cloud Storage bucket to upload to.\n",
    "        project_name (str): project ID for requester pays billing.\n",
    "        raise_threshold (float): file size below which an exception should \n",
    "            be raised.\n",
    "        \n",
    "    Returns:\n",
    "        Nothing; logs an info message about the size of the blob.\n",
    "        \n",
    "    Raises:\n",
    "        Exception: if the blob file size is less than the specified threshold\n",
    "    \"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name, user_project=project_name)    \n",
    "    blob = bucket.get_blob(file_name)\n",
    "    if blob.size < raise_threshold:\n",
    "        raise Exception(\n",
    "            f\"{file_name} file size is smaller than expected\")\n",
    "    else:\n",
    "        logging.info(\n",
    "            f\"{file_name} size in GCS is {int(blob.size * 1e-6)}MB\")\n",
    "\n",
    "\n",
    "def merge_allsky_clearsky_data(single_date):\n",
    "    \"\"\"Merge all sky and clear sky data variables from ERA5.\"\"\"\n",
    "    clearsky = xr.open_dataset(f\"{single_date.strftime('%Y%m%d')}_clearsky.nc\")\n",
    "    allsky = xr.open_dataset(f\"{single_date.strftime('%Y%m%d')}.nc\")\n",
    "    data = xr.merge([allsky, clearsky])\n",
    "    data.to_netcdf(f\"{single_date.strftime('%Y%m%d')}_merged.nc\")\n",
    "\n",
    "\n",
    "def ingest_cds_to_gcs(single_date, bucket_name, user_project=None, cleanup=False):\n",
    "    \"\"\"Retrieve data from Copernicus and upload data to Google Cloud Storage.\n",
    "    \n",
    "    Args:\n",
    "        single_date: date object representing day to retrieve data for.\n",
    "        bucket_name: Google Cloud Storage bucket to upload to.\n",
    "        user_project: project ID for requester pays billing.\n",
    "        cleanup: Optionally remove the downloaded data after upload to GCS.\n",
    "        \n",
    "    Returns:\n",
    "        Nothing; downloads a file, uploads it to GCS, and optionally \n",
    "        removes it.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        check_blob_size(f\"{single_date.strftime('%Y%m%d')}.nc\",\n",
    "                        bucket_name,\n",
    "                        project_name=user_project,\n",
    "                        raise_threshold=3.8e+8)\n",
    "        return\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    file_name = f\"{single_date.strftime('%Y%m%d')}_clearsky.nc\"\n",
    "    client = cdsapi.Client(progress=False, quiet=True)\n",
    "    client.retrieve(\n",
    "    \"reanalysis-era5-single-levels\",\n",
    "    {\n",
    "        \"product_type\": \"reanalysis\",\n",
    "        \"variable\": [\n",
    "            \"surface_solar_radiation_downward_clear_sky\", \n",
    "            \"top_net_solar_radiation_clear_sky\",\n",
    "            \"surface_net_solar_radiation_clear_sky\", \n",
    "            \"2m_temperature\"\n",
    "        ],\n",
    "        \"year\": single_date.strftime(\"%Y\"),\n",
    "        \"month\": single_date.strftime(\"%m\"),\n",
    "        \"day\": single_date.strftime(\"%d\"),\n",
    "        \"time\": [\n",
    "            \"00:00\", \"01:00\", \"02:00\",\n",
    "            \"03:00\", \"04:00\", \"05:00\",\n",
    "            \"06:00\", \"07:00\", \"08:00\",\n",
    "            \"09:00\", \"10:00\", \"11:00\",\n",
    "            \"12:00\", \"13:00\", \"14:00\",\n",
    "            \"15:00\", \"16:00\", \"17:00\",\n",
    "            \"18:00\", \"19:00\", \"20:00\",\n",
    "            \"21:00\", \"22:00\", \"23:00\",\n",
    "        ],\n",
    "        \"format\": \"netcdf\",\n",
    "    },\n",
    "    file_name)\n",
    "    \n",
    "    get_file_gcs(f\"{single_date.strftime('%Y%m%d')}.nc\",\n",
    "                bucket_name, project_name=user_project)\n",
    "    merge_allsky_clearsky_data(single_date)\n",
    "\n",
    "    os.remove(f\"{single_date.strftime('%Y%m%d')}.nc\")\n",
    "    os.rename(f\"{single_date.strftime('%Y%m%d')}_merged.nc\",\n",
    "              f\"{single_date.strftime('%Y%m%d')}.nc\")\n",
    "    \n",
    "    put_file_gcs(f\"{single_date.strftime('%Y%m%d')}.nc\", \n",
    "                 bucket_name, project_name=user_project)\n",
    "    \n",
    "    check_blob_size(f\"{single_date.strftime('%Y%m%d')}.nc\", \n",
    "                    bucket_name, project_name=user_project,\n",
    "                    raise_threshold=3.8e+8)\n",
    "    \n",
    "    if cleanup:\n",
    "        os.remove(f\"{single_date.strftime('%Y%m%d')}.nc\")\n",
    "        os.remove(f\"{single_date.strftime('%Y%m%d')}_clearsky.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab1f15-0708-48ec-bb72-750de156aef1",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d77da-2a1b-4b3e-ab6b-fe863c575f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm_joblib(tqdm(total=sum(1 for _ in daterange(start_date, \n",
    "                                                     end_date)))) as pbar:\n",
    "    Parallel(n_jobs=n_jobs, backend=\"multiprocessing\")(\n",
    "        delayed(ingest_cds_to_gcs)(day,\n",
    "                                   hourly_data_bucket,\n",
    "                                   user_project=project_id,\n",
    "                                   cleanup=True)\n",
    "        for day in daterange(start_date, end_date))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m81"
  },
  "kernelspec": {
   "display_name": "Python [conda env:ingest]",
   "language": "python",
   "name": "conda-env-ingest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
